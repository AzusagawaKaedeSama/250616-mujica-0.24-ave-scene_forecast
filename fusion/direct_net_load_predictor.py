"""
åŸºäºæ·±åº¦å­¦ä¹ çš„ç›´æ¥æ€»å‡€è´Ÿè·é¢„æµ‹æ–¹æ³•
é¿å…ç›¸å…³æ€§é—®é¢˜ï¼Œç«¯åˆ°ç«¯å­¦ä¹ æ‰€æœ‰çœä»½çš„å‡€è´Ÿè·æ€»å’Œ
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt


class DirectNetLoadDataset(Dataset):
    """
    ç›´æ¥å‡€è´Ÿè·é¢„æµ‹çš„æ•°æ®é›†
    """
    
    def __init__(self, 
                 load_data: Dict[str, np.ndarray],  # å„çœä»½è´Ÿè·æ•°æ®
                 pv_data: Dict[str, np.ndarray],    # å„çœä»½å…‰ä¼æ•°æ®
                 wind_data: Dict[str, np.ndarray],  # å„çœä»½é£ç”µæ•°æ®
                 weather_data: Optional[np.ndarray] = None,  # å¤©æ°”æ•°æ®
                 sequence_length: int = 96,  # è¾“å…¥åºåˆ—é•¿åº¦(4å¤©)
                 prediction_horizon: int = 96):  # é¢„æµ‹é•¿åº¦(1å¤©)
        
        self.provinces = list(load_data.keys())
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        
        # å‡†å¤‡æ•°æ®
        self.features, self.targets = self._prepare_data(
            load_data, pv_data, wind_data, weather_data
        )
        
        # æ ‡å‡†åŒ–
        self.feature_scaler = StandardScaler()
        self.target_scaler = StandardScaler()
        
        self.features = self.feature_scaler.fit_transform(self.features)
        self.targets = self.target_scaler.fit_transform(self.targets.reshape(-1, 1)).flatten()
    
    def _prepare_data(self, load_data, pv_data, wind_data, weather_data):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        # è®¡ç®—å†å²å‡€è´Ÿè·ï¼ˆç›®æ ‡å˜é‡ï¼‰
        net_loads = {}
        for province in self.provinces:
            net_loads[province] = load_data[province] - pv_data[province] - wind_data[province]
        
        # è®¡ç®—æ€»å‡€è´Ÿè·ï¼ˆç›®æ ‡ï¼‰
        total_net_load = sum(net_loads.values())
        
        # æ„å»ºç‰¹å¾çŸ©é˜µï¼š[è´Ÿè·, å…‰ä¼, é£ç”µ] Ã— çœä»½æ•°
        feature_dim = len(self.provinces) * 3  # æ¯ä¸ªçœä»½3ä¸ªç‰¹å¾
        if weather_data is not None:
            feature_dim += weather_data.shape[1]  # æ·»åŠ å¤©æ°”ç‰¹å¾
        
        n_samples = len(total_net_load) - self.sequence_length - self.prediction_horizon + 1
        
        features = np.zeros((n_samples, self.sequence_length, feature_dim))
        targets = np.zeros((n_samples, self.prediction_horizon))
        
        for i in range(n_samples):
            # è¾“å…¥ç‰¹å¾: å†å²sequence_lengthä¸ªæ—¶é—´ç‚¹çš„æ•°æ®
            feature_idx = 0
            
            # å„çœä»½è´Ÿè·ã€å…‰ä¼ã€é£ç”µæ•°æ®
            for province in self.provinces:
                features[i, :, feature_idx] = load_data[province][i:i+self.sequence_length]
                features[i, :, feature_idx+1] = pv_data[province][i:i+self.sequence_length]
                features[i, :, feature_idx+2] = wind_data[province][i:i+self.sequence_length]
                feature_idx += 3
            
            # å¤©æ°”æ•°æ®
            if weather_data is not None:
                features[i, :, feature_idx:] = weather_data[i:i+self.sequence_length]
            
            # ç›®æ ‡: æœªæ¥prediction_horizonä¸ªæ—¶é—´ç‚¹çš„æ€»å‡€è´Ÿè·
            targets[i, :] = total_net_load[i+self.sequence_length:i+self.sequence_length+self.prediction_horizon]
        
        # é‡å¡‘ç‰¹å¾ä¸º2D (samples, sequence_length * feature_dim)
        features = features.reshape(n_samples, -1)
        
        return features, targets
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return torch.FloatTensor(self.features[idx]), torch.FloatTensor(self.targets[idx])


class DirectNetLoadPredictor(nn.Module):
    """
    ç›´æ¥é¢„æµ‹æ€»å‡€è´Ÿè·çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
    """
    
    def __init__(self, 
                 input_dim: int,
                 hidden_dims: List[int] = [512, 256, 128],
                 output_dim: int = 96,
                 dropout_rate: float = 0.2):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        # æ„å»ºéšè—å±‚
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.BatchNorm1d(hidden_dim)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)


class UncertaintyQuantification:
    """
    ä¸ç¡®å®šæ€§é‡åŒ–æ¨¡å—ï¼ˆåŸºäºæ·±åº¦é›†æˆï¼‰
    """
    
    def __init__(self, n_models: int = 5):
        self.n_models = n_models
        self.models = []
        self.trained = False
    
    def train_ensemble(self, 
                      train_loader: DataLoader,
                      val_loader: DataLoader,
                      input_dim: int,
                      epochs: int = 100):
        """
        è®­ç»ƒæ¨¡å‹é›†æˆ
        """
        self.models = []
        
        for i in range(self.n_models):
            print(f"è®­ç»ƒæ¨¡å‹ {i+1}/{self.n_models}")
            
            # åˆ›å»ºæ¨¡å‹
            model = DirectNetLoadPredictor(input_dim)
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.MSELoss()
            
            # è®­ç»ƒ
            model = self._train_single_model(
                model, train_loader, val_loader, optimizer, criterion, epochs
            )
            
            self.models.append(model)
        
        self.trained = True
    
    def _train_single_model(self, model, train_loader, val_loader, optimizer, criterion, epochs):
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹
        """
        model.train()
        
        for epoch in range(epochs):
            train_loss = 0
            for batch_features, batch_targets in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_features)
                loss = criterion(outputs, batch_targets)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            if epoch % 20 == 0:
                val_loss = self._validate(model, val_loader, criterion)
                print(f"  Epoch {epoch}: Train Loss = {train_loss/len(train_loader):.4f}, "
                      f"Val Loss = {val_loss:.4f}")
        
        return model
    
    def _validate(self, model, val_loader, criterion):
        """
        éªŒè¯æ¨¡å‹
        """
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_features, batch_targets in val_loader:
                outputs = model(batch_features)
                loss = criterion(outputs, batch_targets)
                val_loss += loss.item()
        
        model.train()
        return val_loss / len(val_loader)
    
    def predict_with_uncertainty(self, X: torch.Tensor) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        é¢„æµ‹å¹¶é‡åŒ–ä¸ç¡®å®šæ€§
        
        Returns:
            (å‡å€¼é¢„æµ‹, æ ‡å‡†å·®, æ‰€æœ‰æ¨¡å‹é¢„æµ‹)
        """
        if not self.trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        predictions = []
        
        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(X).numpy()
                predictions.append(pred)
        
        predictions = np.array(predictions)  # (n_models, n_samples, output_dim)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_pred = predictions.mean(axis=0)
        std_pred = predictions.std(axis=0)
        
        return mean_pred, std_pred, predictions


def create_demonstration_data(n_days: int = 30) -> Tuple[Dict, Dict, Dict, np.ndarray]:
    """
    åˆ›å»ºæ¼”ç¤ºæ•°æ®
    """
    n_points = n_days * 96  # 15åˆ†é’Ÿæ•°æ®ç‚¹
    provinces = ['æ±Ÿè‹', 'ä¸Šæµ·', 'æµ™æ±Ÿ', 'å®‰å¾½', 'ç¦å»º']
    
    # åŸºç¡€è´Ÿè·æ¨¡å¼ï¼ˆæ—¥å‘¨æœŸæ€§ï¼‰
    time_points = np.arange(n_points)
    daily_pattern = 0.3 * np.sin(2 * np.pi * time_points / 96) + 0.7
    
    load_data = {}
    pv_data = {}
    wind_data = {}
    
    # å„çœä»½æ•°æ®ï¼ˆæœ‰ç›¸å…³æ€§ï¼‰
    base_loads = {'æ±Ÿè‹': 90000, 'ä¸Šæµ·': 20000, 'æµ™æ±Ÿ': 70000, 'å®‰å¾½': 50000, 'ç¦å»º': 40000}
    
    for province in provinces:
        # è´Ÿè·æ•°æ®ï¼ˆæœ‰æ—¥å‘¨æœŸ + å™ªå£°ï¼‰
        load_data[province] = (
            base_loads[province] * daily_pattern + 
            np.random.normal(0, base_loads[province] * 0.1, n_points)
        )
        
        # å…‰ä¼æ•°æ®ï¼ˆæ—¥é—´é«˜ï¼Œå¤œé—´0ï¼Œä¸è´Ÿè·æœ‰ç›¸å…³æ€§ï¼‰
        pv_base = np.maximum(0, np.sin(2 * np.pi * (time_points % 96) / 96 - np.pi/2))
        pv_data[province] = (
            base_loads[province] * 0.02 * pv_base + 
            np.random.normal(0, base_loads[province] * 0.005, n_points)
        )
        pv_data[province] = np.maximum(0, pv_data[province])  # éè´Ÿ
        
        # é£ç”µæ•°æ®ï¼ˆéšæœºä½†æœ‰æŒç»­æ€§ï¼‰
        wind_noise = np.random.normal(0, 0.1, n_points)
        wind_smooth = np.convolve(wind_noise, np.ones(24)/24, mode='same')  # å¹³æ»‘
        wind_data[province] = (
            base_loads[province] * 0.015 * (0.5 + wind_smooth) + 
            np.random.normal(0, base_loads[province] * 0.002, n_points)
        )
        wind_data[province] = np.maximum(0, wind_data[province])  # éè´Ÿ
    
    # ç®€åŒ–å¤©æ°”æ•°æ®
    weather_data = np.column_stack([
        25 + 10 * np.sin(2 * np.pi * time_points / (96 * 7)) + np.random.normal(0, 2, n_points),  # æ¸©åº¦
        60 + 20 * np.sin(2 * np.pi * time_points / 96) + np.random.normal(0, 5, n_points),  # æ¹¿åº¦
    ])
    
    return load_data, pv_data, wind_data, weather_data


def compare_methods():
    """
    å¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•ä¸æ·±åº¦å­¦ä¹ ç›´æ¥é¢„æµ‹æ–¹æ³•
    """
    print("ğŸš€ æ·±åº¦å­¦ä¹ ç›´æ¥é¢„æµ‹ vs ä¼ ç»ŸåŠ å’Œæ–¹æ³•å¯¹æ¯”")
    print("=" * 70)
    
    # åˆ›å»ºæ•°æ®
    load_data, pv_data, wind_data, weather_data = create_demonstration_data(n_days=30)
    
    # è®¡ç®—çœŸå®æ€»å‡€è´Ÿè·
    provinces = list(load_data.keys())
    true_total_net_load = sum(
        load_data[p] - pv_data[p] - wind_data[p] for p in provinces
    )
    
    print(f"æ•°æ®æ¦‚å†µ:")
    print(f"  æ—¶é—´é•¿åº¦: 30å¤© ({len(true_total_net_load)}ä¸ªæ•°æ®ç‚¹)")
    print(f"  çœä»½æ•°é‡: {len(provinces)}")
    print(f"  æ€»å‡€è´Ÿè·å‡å€¼: {true_total_net_load.mean():.1f} MW")
    print(f"  æ€»å‡€è´Ÿè·æ ‡å‡†å·®: {true_total_net_load.std():.1f} MW")
    
    # æ–¹æ³•1ï¼šä¼ ç»ŸåŠ å’Œæ–¹æ³•ï¼ˆå¿½ç•¥ç›¸å…³æ€§ï¼‰
    print("\n[æ–¹æ³•1] ä¼ ç»ŸåŠ å’Œæ–¹æ³•ï¼ˆå¿½ç•¥ç›¸å…³æ€§ï¼‰")
    provincial_stds = {}
    for province in provinces:
        net_load = load_data[province] - pv_data[province] - wind_data[province]
        provincial_stds[province] = net_load.std()
    
    # é”™è¯¯çš„ç‹¬ç«‹å‡è®¾
    total_std_wrong = np.sqrt(sum(std**2 for std in provincial_stds.values()))
    print(f"  é¢„æµ‹æ ‡å‡†å·®(ç‹¬ç«‹å‡è®¾): {total_std_wrong:.1f} MW")
    print(f"  å®é™…æ€»ä½“æ ‡å‡†å·®: {true_total_net_load.std():.1f} MW")
    print(f"  è¯¯å·®: {abs(total_std_wrong - true_total_net_load.std()):.1f} MW")
    
    # æ–¹æ³•2ï¼šæ­£ç¡®çš„ç›¸å…³æ€§æ–¹æ³•
    print("\n[æ–¹æ³•2] è€ƒè™‘ç›¸å…³æ€§çš„åˆ†ææ–¹æ³•")
    from correlation_analysis import CorrelatedUncertaintyPropagation
    
    analyzer = CorrelatedUncertaintyPropagation()
    
    # æ„å»ºçœé™…ç›¸å…³æ€§çŸ©é˜µ
    provincial_net_loads = {}
    for province in provinces:
        provincial_net_loads[province] = load_data[province] - pv_data[province] - wind_data[province]
    
    inter_corr_matrix = analyzer.inter_provincial_correlation_analysis(provincial_net_loads)
    
    # è®¡ç®—æ­£ç¡®çš„èšåˆä¸ç¡®å®šæ€§
    provincial_means = {p: provincial_net_loads[p].mean() for p in provinces}
    provincial_stds = {p: provincial_net_loads[p].std() for p in provinces}
    
    total_mean_corr, total_std_corr = analyzer.aggregate_with_correlation(
        provincial_means, provincial_stds, inter_corr_matrix
    )
    
    print(f"  é¢„æµ‹å‡å€¼: {total_mean_corr:.1f} MW")
    print(f"  é¢„æµ‹æ ‡å‡†å·®(è€ƒè™‘ç›¸å…³æ€§): {total_std_corr:.1f} MW")
    print(f"  å®é™…æ€»ä½“æ ‡å‡†å·®: {true_total_net_load.std():.1f} MW")
    print(f"  è¯¯å·®: {abs(total_std_corr - true_total_net_load.std()):.1f} MW")
    
    # æ–¹æ³•3ï¼šæ·±åº¦å­¦ä¹ ç›´æ¥é¢„æµ‹
    print("\n[æ–¹æ³•3] æ·±åº¦å­¦ä¹ ç›´æ¥é¢„æµ‹")
    print("  (æ³¨: å®é™…è®­ç»ƒéœ€è¦æ›´é•¿æ—¶é—´ï¼Œè¿™é‡Œå±•ç¤ºæ¡†æ¶)")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = DirectNetLoadDataset(
        load_data, pv_data, wind_data, weather_data,
        sequence_length=96, prediction_horizon=96
    )
    
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(dataset)}")
    print(f"  è¾“å…¥ç‰¹å¾ç»´åº¦: {dataset.features.shape[1]}")
    
    # ä¼˜åŠ¿åˆ†æ
    print("\nğŸ“Š æ–¹æ³•å¯¹æ¯”æ€»ç»“:")
    print("=" * 70)
    print("æ–¹æ³•                    | ä¸ç¡®å®šæ€§é¢„æµ‹è¯¯å·® | ä¸»è¦ä¼˜åŠ¿")
    print("-" * 70)
    print(f"ä¼ ç»ŸåŠ å’Œ(ç‹¬ç«‹å‡è®¾)       | {abs(total_std_wrong - true_total_net_load.std()):.1f} MW          | ç®€å•å¿«é€Ÿ")
    print(f"ç›¸å…³æ€§åˆ†ææ–¹æ³•          | {abs(total_std_corr - true_total_net_load.std()):.1f} MW          | ç†è®ºæ­£ç¡®")
    print("æ·±åº¦å­¦ä¹ ç›´æ¥é¢„æµ‹        | å¾…è®­ç»ƒéªŒè¯        | ç«¯åˆ°ç«¯å­¦ä¹ ")
    
    print("\nğŸ’¡ æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç‹¬ç‰¹ä¼˜åŠ¿:")
    print("  1. è‡ªåŠ¨å­¦ä¹ æ‰€æœ‰å˜é‡é—´çš„å¤æ‚éçº¿æ€§å…³ç³»")
    print("  2. ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œæ— éœ€æ‰‹å·¥è®¾è®¡ç›¸å…³æ€§çŸ©é˜µ")
    print("  3. å¯ä»¥é›†æˆå¤©æ°”ã€æ—¶é—´ç­‰å¤šç§å¤–éƒ¨ç‰¹å¾")
    print("  4. é€šè¿‡æ¨¡å‹é›†æˆè‡ªç„¶é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§")
    print("  5. å¯ä»¥å¤„ç†æ¦‚å¿µæ¼‚ç§»å’Œåˆ†å¸ƒå˜åŒ–")


if __name__ == "__main__":
    compare_methods() 