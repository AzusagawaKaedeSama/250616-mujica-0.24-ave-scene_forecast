"""
ç›¸å…³å˜é‡ä¸ç¡®å®šæ€§ä¼ æ’­çš„æ­£ç¡®å¤„ç†æ–¹æ³•
åŸºäºåæ–¹å·®çŸ©é˜µå’Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
"""

import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, Tuple, List
import matplotlib.pyplot as plt
import seaborn as sns


class CorrelatedUncertaintyPropagation:
    """
    å¤„ç†ç›¸å…³å˜é‡çš„ä¸ç¡®å®šæ€§ä¼ æ’­
    """
    
    def __init__(self):
        self.correlation_matrices = {}
        self.historical_data = {}
    
    def estimate_correlations_from_data(self, 
                                      load_data: Dict[str, np.ndarray],
                                      pv_data: Dict[str, np.ndarray],
                                      wind_data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        ä»å†å²æ•°æ®ä¼°è®¡ç›¸å…³æ€§çŸ©é˜µ
        
        Args:
            load_data: å„çœä»½è´Ÿè·å†å²æ•°æ®
            pv_data: å„çœä»½å…‰ä¼å†å²æ•°æ®  
            wind_data: å„çœä»½é£ç”µå†å²æ•°æ®
            
        Returns:
            å„çœä»½çš„ç›¸å…³æ€§çŸ©é˜µå­—å…¸
        """
        correlation_matrices = {}
        
        for province in load_data.keys():
            # æ„å»ºè¯¥çœä»½çš„æ•°æ®çŸ©é˜µ [è´Ÿè·, å…‰ä¼, é£ç”µ]
            data_matrix = np.column_stack([
                load_data[province],
                pv_data[province], 
                wind_data[province]
            ])
            
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            corr_matrix = np.corrcoef(data_matrix.T)
            correlation_matrices[province] = corr_matrix
            
            print(f"{province} ç›¸å…³æ€§çŸ©é˜µ:")
            print(f"       è´Ÿè·    å…‰ä¼    é£ç”µ")
            print(f"è´Ÿè·  {corr_matrix[0,0]:.3f}  {corr_matrix[0,1]:.3f}  {corr_matrix[0,2]:.3f}")
            print(f"å…‰ä¼  {corr_matrix[1,0]:.3f}  {corr_matrix[1,1]:.3f}  {corr_matrix[1,2]:.3f}")
            print(f"é£ç”µ  {corr_matrix[2,0]:.3f}  {corr_matrix[2,1]:.3f}  {corr_matrix[2,2]:.3f}")
            print()
        
        return correlation_matrices
    
    def calculate_net_load_uncertainty_correct(self,
                                             load_mean: float, load_std: float,
                                             pv_mean: float, pv_std: float,
                                             wind_mean: float, wind_std: float,
                                             correlation_matrix: np.ndarray) -> Tuple[float, float]:
        """
        æ­£ç¡®è®¡ç®—å‡€è´Ÿè·çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆè€ƒè™‘ç›¸å…³æ€§ï¼‰
        
        å‡€è´Ÿè· = è´Ÿè· - å…‰ä¼ - é£ç”µ
        
        Args:
            load_mean, load_std: è´Ÿè·çš„å‡å€¼å’Œæ ‡å‡†å·®
            pv_mean, pv_std: å…‰ä¼çš„å‡å€¼å’Œæ ‡å‡†å·®
            wind_mean, wind_std: é£ç”µçš„å‡å€¼å’Œæ ‡å‡†å·®
            correlation_matrix: 3x3ç›¸å…³æ€§çŸ©é˜µ [è´Ÿè·, å…‰ä¼, é£ç”µ]
            
        Returns:
            (å‡€è´Ÿè·å‡å€¼, å‡€è´Ÿè·æ ‡å‡†å·®)
        """
        # å‡€è´Ÿè·å‡å€¼ = è´Ÿè·å‡å€¼ - å…‰ä¼å‡å€¼ - é£ç”µå‡å€¼
        net_load_mean = load_mean - pv_mean - wind_mean
        
        # æ„å»ºæ–¹å·®å‘é‡
        variances = np.array([load_std**2, pv_std**2, wind_std**2])
        
        # æ„å»ºæƒé‡å‘é‡ [1, -1, -1] (å‡€è´Ÿè· = è´Ÿè· - å…‰ä¼ - é£ç”µ)
        weights = np.array([1, -1, -1])
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        std_vector = np.array([load_std, pv_std, wind_std])
        cov_matrix = np.outer(std_vector, std_vector) * correlation_matrix
        
        # å‡€è´Ÿè·æ–¹å·® = w^T * Î£ * wï¼Œå…¶ä¸­wæ˜¯æƒé‡å‘é‡ï¼ŒÎ£æ˜¯åæ–¹å·®çŸ©é˜µ
        net_load_variance = weights.T @ cov_matrix @ weights
        net_load_std = np.sqrt(max(0, net_load_variance))  # ç¡®ä¿éè´Ÿ
        
        return net_load_mean, net_load_std
    
    def monte_carlo_simulation(self,
                             means: np.ndarray,
                             stds: np.ndarray, 
                             correlation_matrix: np.ndarray,
                             n_samples: int = 10000) -> Tuple[float, float, np.ndarray]:
        """
        è’™ç‰¹å¡æ´›æ¨¡æ‹ŸéªŒè¯ç›¸å…³å˜é‡çš„ä¸ç¡®å®šæ€§ä¼ æ’­
        
        Args:
            means: å‡å€¼å‘é‡ [è´Ÿè·å‡å€¼, å…‰ä¼å‡å€¼, é£ç”µå‡å€¼]
            stds: æ ‡å‡†å·®å‘é‡ [è´Ÿè·æ ‡å‡†å·®, å…‰ä¼æ ‡å‡†å·®, é£ç”µæ ‡å‡†å·®]
            correlation_matrix: ç›¸å…³æ€§çŸ©é˜µ
            n_samples: æ¨¡æ‹Ÿæ ·æœ¬æ•°
            
        Returns:
            (å‡€è´Ÿè·å‡å€¼, å‡€è´Ÿè·æ ‡å‡†å·®, å‡€è´Ÿè·æ ·æœ¬)
        """
        # æ„å»ºåæ–¹å·®çŸ©é˜µ
        cov_matrix = np.outer(stds, stds) * correlation_matrix
        
        # ç”Ÿæˆå¤šå…ƒæ­£æ€åˆ†å¸ƒæ ·æœ¬
        samples = np.random.multivariate_normal(means, cov_matrix, n_samples)
        
        # è®¡ç®—å‡€è´Ÿè·æ ·æœ¬
        net_load_samples = samples[:, 0] - samples[:, 1] - samples[:, 2]
        
        # ç»Ÿè®¡ç»“æœ
        net_load_mean_mc = np.mean(net_load_samples)
        net_load_std_mc = np.std(net_load_samples)
        
        return net_load_mean_mc, net_load_std_mc, net_load_samples
    
    def inter_provincial_correlation_analysis(self,
                                           provincial_net_loads: Dict[str, np.ndarray]) -> np.ndarray:
        """
        åˆ†æçœé™…å‡€è´Ÿè·ç›¸å…³æ€§
        
        Args:
            provincial_net_loads: å„çœä»½å‡€è´Ÿè·å†å²æ•°æ®
            
        Returns:
            çœé™…ç›¸å…³æ€§çŸ©é˜µ
        """
        provinces = list(provincial_net_loads.keys())
        n_provinces = len(provinces)
        
        # æ„å»ºçœé™…æ•°æ®çŸ©é˜µ
        data_matrix = np.column_stack([provincial_net_loads[p] for p in provinces])
        
        # è®¡ç®—çœé™…ç›¸å…³æ€§çŸ©é˜µ
        inter_corr_matrix = np.corrcoef(data_matrix.T)
        
        # å¯è§†åŒ–ç›¸å…³æ€§çŸ©é˜µ
        plt.figure(figsize=(10, 8))
        sns.heatmap(inter_corr_matrix, 
                   xticklabels=provinces, 
                   yticklabels=provinces,
                   annot=True, fmt='.3f', cmap='coolwarm', center=0)
        plt.title('çœé™…å‡€è´Ÿè·ç›¸å…³æ€§çŸ©é˜µ')
        plt.tight_layout()
        plt.show()
        
        return inter_corr_matrix
    
    def aggregate_with_correlation(self,
                                 provincial_means: Dict[str, float],
                                 provincial_stds: Dict[str, float],
                                 inter_corr_matrix: np.ndarray) -> Tuple[float, float]:
        """
        è€ƒè™‘çœé™…ç›¸å…³æ€§çš„æ­£ç¡®èšåˆæ–¹æ³•
        
        Args:
            provincial_means: å„çœä»½å‡€è´Ÿè·å‡å€¼
            provincial_stds: å„çœä»½å‡€è´Ÿè·æ ‡å‡†å·®
            inter_corr_matrix: çœé™…ç›¸å…³æ€§çŸ©é˜µ
            
        Returns:
            (æ€»å‡€è´Ÿè·å‡å€¼, æ€»å‡€è´Ÿè·æ ‡å‡†å·®)
        """
        provinces = list(provincial_means.keys())
        
        # æ€»å‡å€¼ = å„çœä»½å‡å€¼ä¹‹å’Œ
        total_mean = sum(provincial_means.values())
        
        # æ„å»ºæ ‡å‡†å·®å‘é‡
        std_vector = np.array([provincial_stds[p] for p in provinces])
        
        # æ„å»ºåæ–¹å·®çŸ©é˜µ
        cov_matrix = np.outer(std_vector, std_vector) * inter_corr_matrix
        
        # æƒé‡å‘é‡å…¨ä¸º1ï¼ˆæ±‚å’Œï¼‰
        weights = np.ones(len(provinces))
        
        # æ€»æ–¹å·® = w^T * Î£ * w
        total_variance = weights.T @ cov_matrix @ weights
        total_std = np.sqrt(total_variance)
        
        return total_mean, total_std


def demonstration_example():
    """
    æ¼”ç¤ºæ­£ç¡®çš„ç›¸å…³æ€§å¤„ç†æ–¹æ³•
    """
    print("ğŸ¯ ç›¸å…³å˜é‡ä¸ç¡®å®šæ€§ä¼ æ’­æ¼”ç¤º")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    
    # å‡è®¾å‚æ•°
    load_mean, load_std = 20000, 2000
    pv_mean, pv_std = 500, 150
    wind_mean, wind_std = 300, 100
    
    # å‡è®¾ç›¸å…³æ€§çŸ©é˜µ [è´Ÿè·, å…‰ä¼, é£ç”µ]
    correlation_matrix = np.array([
        [1.0,  0.3, -0.1],  # è´Ÿè·ä¸å…‰ä¼æ­£ç›¸å…³ï¼Œä¸é£ç”µè´Ÿç›¸å…³
        [0.3,  1.0,  0.0],  # å…‰ä¼ä¸é£ç”µæ— ç›¸å…³
        [-0.1, 0.0,  1.0]   # 
    ])
    
    print("å‡è®¾å‚æ•°:")
    print(f"è´Ÿè·: å‡å€¼={load_mean}MW, æ ‡å‡†å·®={load_std}MW")
    print(f"å…‰ä¼: å‡å€¼={pv_mean}MW, æ ‡å‡†å·®={pv_std}MW") 
    print(f"é£ç”µ: å‡å€¼={wind_mean}MW, æ ‡å‡†å·®={wind_std}MW")
    print("\nç›¸å…³æ€§çŸ©é˜µ:")
    print(correlation_matrix)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = CorrelatedUncertaintyPropagation()
    
    # æ–¹æ³•1ï¼šè§£æè®¡ç®—ï¼ˆè€ƒè™‘ç›¸å…³æ€§ï¼‰
    net_mean_correct, net_std_correct = analyzer.calculate_net_load_uncertainty_correct(
        load_mean, load_std, pv_mean, pv_std, wind_mean, wind_std, correlation_matrix
    )
    
    # æ–¹æ³•2ï¼šè’™ç‰¹å¡æ´›éªŒè¯
    means = np.array([load_mean, pv_mean, wind_mean])
    stds = np.array([load_std, pv_std, wind_std])
    net_mean_mc, net_std_mc, samples = analyzer.monte_carlo_simulation(
        means, stds, correlation_matrix, n_samples=100000
    )
    
    # æ–¹æ³•3ï¼šé”™è¯¯çš„ç‹¬ç«‹å‡è®¾
    net_mean_wrong = load_mean - pv_mean - wind_mean  # å‡å€¼è®¡ç®—ç›¸åŒ
    net_std_wrong = np.sqrt(load_std**2 + pv_std**2 + wind_std**2)  # å¿½ç•¥ç›¸å…³æ€§
    
    print("\nğŸ” ç»“æœå¯¹æ¯”:")
    print("=" * 60)
    print(f"æ­£ç¡®æ–¹æ³•(è§£æ): å‡å€¼={net_mean_correct:.1f}MW, æ ‡å‡†å·®={net_std_correct:.1f}MW")
    print(f"è’™ç‰¹å¡æ´›éªŒè¯:   å‡å€¼={net_mean_mc:.1f}MW, æ ‡å‡†å·®={net_std_mc:.1f}MW")
    print(f"é”™è¯¯æ–¹æ³•(ç‹¬ç«‹): å‡å€¼={net_mean_wrong:.1f}MW, æ ‡å‡†å·®={net_std_wrong:.1f}MW")
    
    print(f"\næ ‡å‡†å·®å·®å¼‚: {abs(net_std_correct - net_std_wrong):.1f}MW")
    print(f"ç›¸å¯¹è¯¯å·®: {abs(net_std_correct - net_std_wrong)/net_std_correct*100:.1f}%")
    
    # åŒºé—´å¯¹æ¯”
    confidence_level = 1.96  # 95%ç½®ä¿¡åŒºé—´
    
    print(f"\nğŸ“Š 95%ç½®ä¿¡åŒºé—´å¯¹æ¯”:")
    correct_lower = net_mean_correct - confidence_level * net_std_correct
    correct_upper = net_mean_correct + confidence_level * net_std_correct
    
    wrong_lower = net_mean_wrong - confidence_level * net_std_wrong
    wrong_upper = net_mean_wrong + confidence_level * net_std_wrong
    
    print(f"æ­£ç¡®æ–¹æ³•: [{correct_lower:.1f}, {correct_upper:.1f}] MW")
    print(f"é”™è¯¯æ–¹æ³•: [{wrong_lower:.1f}, {wrong_upper:.1f}] MW")
    print(f"åŒºé—´å®½åº¦å·®å¼‚: {abs((correct_upper-correct_lower) - (wrong_upper-wrong_lower)):.1f}MW")


if __name__ == "__main__":
    demonstration_example() 